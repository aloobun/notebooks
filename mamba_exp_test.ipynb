{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mX3rw4k6Kskp"
      },
      "outputs": [],
      "source": [
        "!pip install causal-conv1d>=1.1.0\n",
        "!pip install mamba-ssm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip setuptools wheel\n",
        "!pip install accelerate transformers wandb\n",
        "!pip install apache-beam\n",
        "!pip install numpy>=1.17 --ignore-installed\n",
        "!pip install git+https://github.com/huggingface/datasets#egg=datasets"
      ],
      "metadata": {
        "id": "vUB1wYvDKxDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import os\n",
        "\n",
        "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
        "from mamba_ssm.models.config_mamba import MambaConfig\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from dataclasses import asdict\n",
        "import json"
      ],
      "metadata": {
        "id": "AkWO70ZAKxFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MambaConfigForTrainer:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.config = MambaConfig(**kwargs)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return asdict(self.config)\n",
        "\n",
        "    def to_json_string(self):\n",
        "        return json.dumps(self.to_dict(), indent=4)\n",
        "\n",
        "    def __getattr__(self, item):\n",
        "        try:\n",
        "            return getattr(self.config, item)\n",
        "        except AttributeError:\n",
        "            raise AttributeError(f\"'MambaConfigForTrainer' object has no attribute '{item}'\")"
      ],
      "metadata": {
        "id": "POQwDYxXKxIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MambaTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        input_ids = inputs.pop(\"input_ids\")\n",
        "        lm_logits = model(input_ids)[0]\n",
        "\n",
        "        labels = input_ids.to(lm_logits.device)\n",
        "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
        "        labels = labels[:, 1:].contiguous()\n",
        "\n",
        "        loss_fct = torch.nn.CrossEntropyLoss()\n",
        "        lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))\n",
        "\n",
        "        return lm_loss"
      ],
      "metadata": {
        "id": "2o8Go8XpKxK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "wQzQ5Jp_KxOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mamba_config = MambaConfigForTrainer(\n",
        "    d_model = 256,\n",
        "    n_layer = 8,\n",
        "    vocab_size = len(tokenizer),\n",
        ")\n",
        "\n",
        "model = MambaLMHeadModel(\n",
        "    config = mamba_config,\n",
        "    device = \"cuda\",\n",
        ")"
      ],
      "metadata": {
        "id": "I8qNma4tKxQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_dataset = load_dataset(\"JeanKaddour/minipile\")\n",
        "wiki_dataset"
      ],
      "metadata": {
        "id": "KC_ZcdyEKxTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], return_special_tokens_mask=True)\n",
        "\n",
        "tokenized_datasets = wiki_dataset.map(tokenize_function, batched=True)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "AqzDHmpbKxWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install wandb\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./checkpoints\",\n",
        "    report_to=\"wandb\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=10,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "trainer = MambaTrainer(\n",
        "    args=args,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        ")"
      ],
      "metadata": {
        "id": "ZdyRURB-KxZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "YiOiaNztKxbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./mamba-1\")"
      ],
      "metadata": {
        "id": "AOPbJOfmKxfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZXGmx37jKxh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DPUTkzIXKxk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hO4_B8pdKxnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nBmpfvATKxqm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}