{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nimport torch\nimport torch.nn as nn\nfrom datasets import load_dataset, Audio\nfrom transformers import EncodecModel, AutoProcessor, TrainingArguments, Trainer\nimport numpy as np\n\nMODEL_ID = \"facebook/encodec_24khz\"\nDATASET_ID = \"hf-internal-testing/librispeech_asr_dummy\" #example\nSPLIT = \"validation\"\nMAX_DURATION_SAMPLES = 123840\nOUTPUT_DIR = \"./results_encodec_fixed_2\"\nLEARNING_RATE = 2e-5\nBATCH_SIZE = 4\nNUM_EPOCHS = 3\n\nprocessor = AutoProcessor.from_pretrained(MODEL_ID)\ntarget_sampling_rate = processor.sampling_rate\n\nlibrispeech_dummy = load_dataset(DATASET_ID, split=SPLIT)\nlibrispeech_dummy = librispeech_dummy.cast_column(\n    \"audio\", Audio(sampling_rate=target_sampling_rate)\n)\n\nclass CustomEncodecModelForReconstruction(EncodecModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.loss_fn = nn.MSELoss()\n\n    def forward(\n        self,\n        input_values,\n        attention_mask=None,\n        labels=None,\n        return_dict=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Forward pass for reconstruction. Accepts 'attention_mask' from Trainer.\n        Does not pass output_attentions/hidden_states to base model.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        outputs = super().forward(\n            input_values=input_values,\n            padding_mask=attention_mask,\n            return_dict=True, # Request dict output from base model\n            # Removed output_attentions and output_hidden_states from the call\n        )\n\n        reconstructed_audio = outputs.audio_values\n\n        loss = None\n        if labels is not None:\n            output_length = reconstructed_audio.shape[-1]\n            label_length = labels.shape[-1]\n\n            if output_length > label_length:\n                padding_size = output_length - label_length\n                padded_labels = nn.functional.pad(labels, (0, padding_size))\n            elif label_length > output_length:\n                 padded_labels = labels[..., :output_length]\n            else:\n                 padded_labels = labels\n\n            if padded_labels.dim() == 2:\n                padded_labels = padded_labels.unsqueeze(1)\n\n            mask = attention_mask\n            if mask is not None:\n                mask_length = mask.shape[-1]\n                if output_length > mask_length:\n                    mask_padding_size = output_length - mask_length\n                    mask = nn.functional.pad(mask, (0, mask_padding_size), value=0)\n                elif mask_length > output_length:\n                    mask = mask[..., :output_length]\n\n                if mask.dim() == 2:\n                     mask = mask.unsqueeze(1)\n\n                mask = mask.bool()\n\n\n                loss = self.loss_fn(\n                    reconstructed_audio.where(mask, torch.tensor(0.0, device=reconstructed_audio.device)),\n                    padded_labels.where(mask, torch.tensor(0.0, device=padded_labels.device))\n                )\n            else:\n                 loss = self.loss_fn(reconstructed_audio, padded_labels)\n\n        final_outputs = {}\n        if loss is not None:\n            final_outputs[\"loss\"] = loss\n\n        for key, value in outputs.items():\n            final_outputs[key] = value\n\n        if not return_dict:\n             output_tuple_values = [v for k, v in final_outputs.items() if k != 'loss' and v is not None]\n             return (loss,) + tuple(output_tuple_values) if loss is not None else tuple(output_tuple_values)\n        else:\n            return final_outputs\n\n\nmodel = CustomEncodecModelForReconstruction.from_pretrained(MODEL_ID)\n\ndef preprocess_function(examples, max_length=MAX_DURATION_SAMPLES):\n    try:\n        audio_data = examples[\"audio\"][\"array\"]\n        current_sampling_rate = examples[\"audio\"][\"sampling_rate\"]\n    except Exception as e:\n        print(f\"Error accessing audio data for example: {examples}. Error: {e}\")\n        return {\"input_values\": [], \"attention_mask\": [], \"labels\": []}\n\n    if current_sampling_rate != target_sampling_rate:\n        raise ValueError(f\"Incorrect sampling rate: {current_sampling_rate} vs {target_sampling_rate}\")\n\n    if len(audio_data) > max_length:\n        processed_audio = audio_data[:max_length]\n    else:\n        padding = np.zeros(max_length - len(audio_data))\n        processed_audio = np.concatenate([audio_data, padding])\n\n    processed_audio = processed_audio.astype(np.float32)\n\n    processed = processor(\n        raw_audio=processed_audio,\n        sampling_rate=target_sampling_rate,\n        return_tensors=\"pt\"\n    )\n\n    input_values = torch.squeeze(processed.input_values, 0)\n\n    padding_mask = processed.get(\"padding_mask\")\n    if padding_mask is not None:\n         mask = torch.squeeze(padding_mask, 0)\n    else:\n         mask = torch.ones_like(input_values[0], dtype=torch.long) # Assuming input_values is [1, T] after squeeze\n\n    labels = torch.tensor(processed_audio, dtype=torch.float32)\n\n    output = {\"input_values\": input_values, \"attention_mask\": mask, \"labels\": labels}\n    return output\n\ntokenized_datasets = librispeech_dummy.map(\n    preprocess_function,\n    remove_columns=librispeech_dummy.column_names,\n    num_proc=1\n)\ntokenized_datasets = tokenized_datasets.filter(lambda example: \"input_values\" in example and len(example['input_values']) > 0)\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    eval_strategy=\"epoch\",\n    learning_rate=LEARNING_RATE,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    num_train_epochs=NUM_EPOCHS,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"none\",\n    remove_unused_columns=False,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets,\n    eval_dataset=tokenized_datasets,\n)\n\nprint(\"Starting training...\")\ntrainer.train()\nprint(\"Training finished.\")\n\ntrainer.save_model(f\"{OUTPUT_DIR}/final_model\")\nprocessor.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\nprint(f\"Final model saved to {OUTPUT_DIR}/final_model\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}