{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7107b3e390>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaModel, LlamaConfig, LlamaForCausalLM, LlamaForSequenceClassification\n",
    "\n",
    "torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LlamaConfig(\n",
    "    vocab_size=100,\n",
    "    hidden_size=256,\n",
    "    intermediate_size=512,\n",
    "    num_hidden_layers=2,\n",
    "    num_attention_heads=4,\n",
    "    num_key_value_heads=4,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM(config)\n",
    "\n",
    "model.save_pretrained(\"./lm_pretrained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at ./lm_pretrained and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#1 label sequence classification\n",
    "rm_model = LlamaForSequenceClassification.from_pretrained(\"./lm_pretrained\", num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForSequenceClassification(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(100, 256)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=256, out_features=512, bias=False)\n",
      "          (up_proj): Linear(in_features=256, out_features=512, bias=False)\n",
      "          (down_proj): Linear(in_features=512, out_features=256, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (score): Linear(in_features=256, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen reward :  0.47253820300102234\n",
      "rejected reward :  0.21928271651268005\n",
      "model loss: 0.5745154023170471\n"
     ]
    }
   ],
   "source": [
    "x_chosen = torch.randint(0, 100, (1,10))\n",
    "x_rejected = torch.randint(0,100, (1,10))\n",
    "\n",
    "margin = 3.0\n",
    "\n",
    "idx={}\n",
    "idx['input_ids']= x_chosen\n",
    "rm_chosen = rm_model(**idx).logits\n",
    "\n",
    "idx['input_ids']= x_rejected\n",
    "rm_rejected = rm_model(**idx).logits\n",
    "\n",
    "loss = -torch.sigmoid(rm_chosen-rm_rejected).log()\n",
    "\n",
    "print(f'chosen reward :  {rm_chosen.item()}')\n",
    "print(f'rejected reward :  {rm_rejected.item()}')\n",
    "print(f'model loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with margin: -2.808908224105835\n"
     ]
    }
   ],
   "source": [
    "loss_with_margin=torch.sigmoid(rm_chosen-rm_rejected-margin).log()\n",
    "print(f'with margin: {loss_with_margin.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with margin: -2.808908224105835\n"
     ]
    }
   ],
   "source": [
    "print(f'with margin: {loss_with_margin.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3\n"
     ]
    }
   ],
   "source": [
    "#double reward!!!\n",
    "\n",
    "def llama_select_dreward(reward_s,reward_h):\n",
    "    return reward_s if reward_s<0.15 else reward_h\n",
    "\n",
    "rc = llama_select_dreward(reward_s=-0.3, reward_h=0.7)\n",
    "print(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_sigmoid(x):\n",
    "    return torch.log(x / (1 - x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.1972])\n"
     ]
    }
   ],
   "source": [
    "sigmoid_o = torch.tensor([0.9])\n",
    "inverse_sigmoid_o = inverse_sigmoid(sigmoid_o)\n",
    "print(inverse_sigmoid_o) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4055])\n"
     ]
    }
   ],
   "source": [
    "sigmoid_o = torch.tensor([0.4])\n",
    "inverse_sigmoid_o = inverse_sigmoid(sigmoid_o)\n",
    "print(inverse_sigmoid_o) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whiten(values: torch.Tensor, shift_mean: bool = True) -> torch.Tensor:\n",
    "    mean, var = torch.mean(values), torch.var(values)\n",
    "    whitened = (values-mean) * torch.rsqrt(var + 1e-8) \n",
    "    if not shift_mean:\n",
    "        whitened += mean\n",
    "    return whitened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8300, 1.2000, 2.2000, 4.5000]])\n",
      "tensor([[-0.8198, -0.5955,  0.0106,  1.4047]])\n"
     ]
    }
   ],
   "source": [
    "values = torch.Tensor([[0.830, 1.200, 2.200, 4.500]])\n",
    "values_w = whiten(values)\n",
    "print(values)\n",
    "print(values_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kl penalty\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#actor model\n",
    "model = LlamaForCausalLM(config)\n",
    "#reference model\n",
    "model_old = LlamaForCausalLM(config)\n",
    "\n",
    "#old polcy\n",
    "index_old = torch.randint(0, 100, (1,1))\n",
    "prob_old = torch.rand(1,1)\n",
    "print('old policy idx: ', index_old.item())\n",
    "print('old policy prob: ', prob_old.item())\n",
    "\n",
    "#new policy\n",
    "x = torch.randint(0, 100, (1,10))\n",
    "output = model(x)['logits'][:,-1,:].sigmoid()\n",
    "prob = torch.gather(output, dim=1, index=index_old)\n",
    "print('policy prob:', prob.item())\n",
    "\n",
    "#kl div\n",
    "kl = F.kl_div(torch.log(prob), prob_old)\n",
    "print('kl penalty: ',kl.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm_score 0.21928271651268005\n",
      "rm_core with kl 0.2197381854057312\n"
     ]
    }
   ],
   "source": [
    "#reward for ppo\n",
    "beta = 0.01\n",
    "rm_score = rm_model(**idx).logits\n",
    "rm_ppo = rm_score - beta * kl\n",
    "print('rm_score', rm_score.item())\n",
    "print('rm_core with kl', rm_ppo.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smalls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
